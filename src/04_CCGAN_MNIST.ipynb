{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCGANs - Context-Conditional Generative Adversarial Networks\n",
    "\n",
    "Introduction to Context-Conditional Generative Adversarial Networks or CCGANs.\n",
    "\n",
    "This notebook is organized follows:\n",
    "\n",
    "1. **Background**\n",
    "* **Definition**\n",
    "* **Training CCGANs with MNIST dataset, Keras and TensorFlow**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background\n",
    "\n",
    "**Generative adversarial nets** consists of two models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the probability that a sample came from the training data rather than $G$.\n",
    "\n",
    "The generator distribution $p_g$ over data data $x$, the generator builds a mapping function from a prior noise distribution $p_z(z)$ to data space as $G(z;\\theta_g)$.\n",
    "\n",
    "The discriminator, $D(x;\\theta_d)$, outputs a single scalar representing the probability that $x$ came form training data rather than $p_g$.\n",
    "\n",
    "The value function $V(G,D)$:\n",
    "\n",
    "$$ \\underset{G}{min} \\: \\underset{D}{max} \\; V(D,G) = \\mathbb{E}_{x\\sim p_{data}(x)}[log D(x)] + \\mathbb{E}_{z\\sim p_{z}(z)}[log(1 - D(G(z)))]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definition\n",
    "\n",
    "Context-Conditional Generative Adversarial Networks (CC-GANs) are conditional GANs where the generator is trained to fill in a missing image patch and the generator and discriminator are conditioned on the surrounding pixels.\n",
    "\n",
    "CC-GANs address a different task: determining if a part of an image is real or fake given the surrounding context.\n",
    "\n",
    "### Generator and Discriminator\n",
    "The generator $G$ receives as input an image with a randomly masked out patch. The generator outputs an entire image.  We fill in the missing patch from the generated output and then pass the completed image into $D$.\n",
    "\n",
    "### Value function\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\underset{G}{min} \\: \\underset{D}{max} \\; V(D,G) =& \\mathbb{E}_{x\\sim \\mathcal{X}}[log D(x)] + \\mathbb{E}_{x\\sim \\mathcal{X}, m\\sim \\mathcal{M}}[log(1 - D(x_I))] \\\\\n",
    "    x_I =& (1 - m) \\bigodot x_G + m \\bigodot x \\\\\n",
    "    x_G =& G(m \\bigodot x, z)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Structure\n",
    "\n",
    "![ccgan](../img/ccgan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training CCGANs with MINIST dataset, Keras and TensorFlow\n",
    "\n",
    "CCGANs implementation using \"U-net\" model and convolutional neural network and the [Keras](https://keras.io/) library.\n",
    "\n",
    "### 1. Load data\n",
    "\n",
    "#### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:33:56.101206Z",
     "start_time": "2018-07-30T22:33:55.687042Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:34:02.366197Z",
     "start_time": "2018-07-30T22:33:56.103100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LeakyReLU, BatchNormalization\n",
    "from keras.layers import Input, Flatten, Embedding, multiply, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras import initializers\n",
    "\n",
    "from keras_contrib.layers.normalization import InstanceNormalization\n",
    "from keras.layers import Concatenate, GaussianNoise\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras import losses\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "\n",
    "# from scipy.misc import imresize\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:34:03.147557Z",
     "start_time": "2018-07-30T22:34:02.368731Z"
    }
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore visual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:34:03.595736Z",
     "start_time": "2018-07-30T22:34:03.149569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    x_y = X_train[y_train == i]\n",
    "    plt.imshow(x_y[0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Class %d\" % (i))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping and normalizing the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:34:03.987317Z",
     "start_time": "2018-07-30T22:34:03.597699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (60000, 28, 28)\n",
      "y_train.shape (60000,)\n",
      "X_train reshape: (60000, 28, 28, 1)\n",
      "y_train reshape: (60000, 11)\n"
     ]
    }
   ],
   "source": [
    "print('X_train.shape', X_train.shape)\n",
    "print('y_train.shape', y_train.shape)\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "    input_shape = (1, 28, 28)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "    input_shape = (28, 28, 1)\n",
    "\n",
    "# the generator is using tanh activation, for which we need to preprocess \n",
    "# the image data into the range between -1 and 1.\n",
    "\n",
    "X_train = np.float32(X_train)\n",
    "X_train = (X_train / 255 - 0.5) * 2\n",
    "X_train = np.clip(X_train, -1, 1)\n",
    "\n",
    "# y to categorical\n",
    "num_classes = 10\n",
    "y_train = to_categorical(y_train, num_classes=num_classes+1)\n",
    "\n",
    "print('X_train reshape:', X_train.shape)\n",
    "print('y_train reshape:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:34:25.981237Z",
     "start_time": "2018-07-30T22:34:03.989904Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train reshape: (60000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = resize(X_train, [X_train.shape[0], 32, 32, 1])\n",
    "print('X_train reshape:', X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define model\n",
    "\n",
    "#### Generator\n",
    "\n",
    "\"U-Net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:34:27.119130Z",
     "start_time": "2018-07-30T22:34:25.983624Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of filters in first layer of generator\n",
    "gf = 32\n",
    "k = 4\n",
    "s = 2\n",
    "\n",
    "# imagem shape 28x28x1\n",
    "img_shape = X_train[0].shape\n",
    "\n",
    "# Generator input\n",
    "img_g = Input(shape=(img_shape))\n",
    "\n",
    "# Downsampling\n",
    "d1 = Conv2D(gf, kernel_size=k, strides=s, padding='same')(img_g)\n",
    "d1 = LeakyReLU(alpha=0.2)(d1)\n",
    "\n",
    "d2 = Conv2D(gf*2, kernel_size=k, strides=s, padding='same')(d1)\n",
    "d2 = LeakyReLU(alpha=0.2)(d2)\n",
    "d2 = BatchNormalization(momentum=0.8)(d2)\n",
    "\n",
    "d3 = Conv2D(gf*4, kernel_size=k, strides=s, padding='same')(d2)\n",
    "d3 = LeakyReLU(alpha=0.2)(d3)\n",
    "d3 = BatchNormalization(momentum=0.8)(d3)\n",
    "\n",
    "d4 = Conv2D(gf*8, kernel_size=k, strides=s, padding='same')(d3)\n",
    "d4 = LeakyReLU(alpha=0.2)(d4)\n",
    "d4 = BatchNormalization(momentum=0.8)(d4)\n",
    "\n",
    "# Upsampling\n",
    "u1 = UpSampling2D(size=2)(d4)\n",
    "u1 = Conv2D(gf*4, kernel_size=k, strides=1, padding='same', activation='relu')(u1)\n",
    "u1 = BatchNormalization(momentum=0.8)(u1)\n",
    "\n",
    "u2 = Concatenate()([u1, d3])\n",
    "u2 = UpSampling2D(size=2)(u2)\n",
    "u2 = Conv2D(gf*2, kernel_size=k, strides=1, padding='same', activation='relu')(u2)\n",
    "u2 = BatchNormalization(momentum=0.8)(u2)\n",
    "\n",
    "u3 = Concatenate()([u2, d2])\n",
    "u3 = UpSampling2D(size=2)(u3)\n",
    "u3 = Conv2D(gf, kernel_size=k, strides=1, padding='same', activation='relu')(u3)\n",
    "u3 = BatchNormalization(momentum=0.8)(u3)\n",
    "\n",
    "u4 = Concatenate()([u3, d1])\n",
    "u4 = UpSampling2D(size=2)(u4)\n",
    "u4 = Conv2D(1, kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n",
    "\n",
    "generator = Model(img_g, u4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator model visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:34:27.128443Z",
     "start_time": "2018-07-30T22:34:27.121240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 16, 32)   544         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 16, 16, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 8, 8, 64)     32832       leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 8, 8, 64)     0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 8, 8, 64)     256         leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 4, 4, 128)    131200      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 4, 4, 128)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 4, 4, 128)    512         leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 2, 2, 256)    524544      batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 2, 2, 256)    0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 2, 2, 256)    1024        leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 4, 4, 256)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 4, 4, 128)    524416      up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 4, 4, 128)    512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4, 4, 256)    0           batch_normalization_4[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 8, 8, 256)    0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 8, 8, 64)     262208      up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 8, 8, 64)     256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 8, 8, 128)    0           batch_normalization_5[0][0]      \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 16, 16, 128)  0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 32)   65568       up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 32)   128         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 16, 16, 64)   0           batch_normalization_6[0][0]      \n",
      "                                                                 leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 32, 32, 64)   0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 1)    1025        up_sampling2d_4[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,545,025\n",
      "Trainable params: 1,543,681\n",
      "Non-trainable params: 1,344\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# prints a summary representation of your model\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator\n",
    "\n",
    "Our discriminator is a **convolutional neural network** that takes a 28x28 image with 1 channel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:34:27.281140Z",
     "start_time": "2018-07-30T22:34:27.130735Z"
    }
   },
   "outputs": [],
   "source": [
    "# Discriminator network\n",
    "k = 4\n",
    "\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Conv2D(64, kernel_size=k, strides=2, padding='same', input_shape=img_shape))\n",
    "discriminator.add(LeakyReLU(alpha=0.8))\n",
    "discriminator.add(Conv2D(128, kernel_size=k, strides=2, padding='same'))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(InstanceNormalization())\n",
    "discriminator.add(Conv2D(256, kernel_size=k, strides=2, padding='same'))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(InstanceNormalization())\n",
    "\n",
    "img_d = Input(shape=img_shape)\n",
    "features = discriminator(img_d)\n",
    "\n",
    "validity = Conv2D(1, kernel_size=k, strides=1, padding='same')(features)\n",
    "# validity = Flatten()(validity)\n",
    "# validity = Dense(1, activation='sigmoid')(validity)\n",
    "\n",
    "label = Flatten()(features)\n",
    "label = Dense(num_classes+1, activation=\"softmax\")(label)\n",
    "\n",
    "discriminator = Model(img_d, [validity, label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator model visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:34:27.287001Z",
     "start_time": "2018-07-30T22:34:27.283325Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 32, 32, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 4, 4, 256)    656836      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4096)         0           sequential_1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 4, 4, 1)      4097        sequential_1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 11)           45067       flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 706,000\n",
      "Trainable params: 706,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# prints a summary representation of your model\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compile model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:34:27.340948Z",
     "start_time": "2018-07-30T22:34:27.289347Z"
    }
   },
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "\n",
    "discriminator.compile(opt, loss=['mse', 'categorical_crossentropy'],\n",
    "                      loss_weights=[0.5, 0.5],\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:34:27.896553Z",
     "start_time": "2018-07-30T22:34:27.342772Z"
    }
   },
   "outputs": [],
   "source": [
    "# The generator takes noise as input and generates imgs\n",
    "masked_img = Input(shape=(img_shape))\n",
    "gen_img = generator(masked_img)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "validity, _ = discriminator(gen_img)\n",
    "\n",
    "d_g = Model(masked_img, validity)\n",
    "\n",
    "d_g.compile(opt, loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:34:27.903476Z",
     "start_time": "2018-07-30T22:34:27.898516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 32, 32, 1)         1545025   \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              [(None, 4, 4, 1), (None,  706000    \n",
      "=================================================================\n",
      "Total params: 2,251,025\n",
      "Trainable params: 1,543,681\n",
      "Non-trainable params: 707,344\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# prints a summary representation of your model\n",
    "d_g.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:34:27.910509Z",
     "start_time": "2018-07-30T22:34:27.905760Z"
    }
   },
   "outputs": [],
   "source": [
    "def mask_randomly(imgs, mask_width=10, mask_height=10):\n",
    "    y1 = np.random.randint(0, imgs.shape[1] - mask_height, imgs.shape[0])\n",
    "    y2 = y1 + mask_height\n",
    "    x1 = np.random.randint(0, imgs.shape[2] - mask_width, imgs.shape[0])\n",
    "    x2 = x1 + mask_width\n",
    "\n",
    "    masked_imgs = np.empty_like(imgs)\n",
    "    for i, img in enumerate(imgs):\n",
    "        masked_img = img.copy()\n",
    "        _y1, _y2, _x1, _x2 = y1[i], y2[i], x1[i], x2[i],\n",
    "        masked_img[_y1:_y2, _x1:_x2, :] = 0\n",
    "        masked_imgs[i] = masked_img\n",
    "\n",
    "    return masked_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:34:28.185017Z",
     "start_time": "2018-07-30T22:34:27.912817Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mask_randomly(X_train[0:1])[0].reshape(32, 32))\n",
    "plt.show()\n",
    "plt.imshow(mask_randomly(X_train[0:1])[0].reshape(32, 32))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fit model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:36:50.321540Z",
     "start_time": "2018-07-30T22:34:28.186878Z"
    }
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[4,4,128,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: conv2d_11/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=1150973, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_4/random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'conv2d_11/random_uniform/RandomUniform', defined at:\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/tornado/ioloop.py\", line 759, in _run_callback\n    ret = callback()\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-54fd145beede>\", line 10, in <module>\n    discriminator.add(Conv2D(256, kernel_size=k, strides=2, padding='same'))\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/keras/engine/sequential.py\", line 187, in add\n    output_tensor = layer(self.outputs[0])\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 432, in __call__\n    self.build(input_shapes[0])\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/keras/layers/convolutional.py\", line 138, in build\n    constraint=self.kernel_constraint)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 249, in add_weight\n    weight = K.variable(initializer(shape),\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/keras/initializers.py\", line 218, in __call__\n    dtype=dtype, seed=self.seed)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 4077, in random_uniform\n    dtype=dtype, seed=seed)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/tensorflow/python/ops/random_ops.py\", line 240, in random_uniform\n    shape, dtype, seed=seed1, seed2=seed2)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/tensorflow/python/ops/gen_random_ops.py\", line 449, in _random_uniform\n    name=name)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[4,4,128,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: conv2d_11/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=1150973, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_4/random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/generative/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/generative/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/generative/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/generative/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[4,4,128,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: conv2d_11/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=1150973, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_4/random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b373589e7b93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mreal_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0md_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Fake Samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/generative/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1218\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/generative/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2652\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2653\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2655\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/generative/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/generative/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/generative/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/generative/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/generative/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[4,4,128,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: conv2d_11/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=1150973, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_4/random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'conv2d_11/random_uniform/RandomUniform', defined at:\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/tornado/ioloop.py\", line 759, in _run_callback\n    ret = callback()\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-54fd145beede>\", line 10, in <module>\n    discriminator.add(Conv2D(256, kernel_size=k, strides=2, padding='same'))\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/keras/engine/sequential.py\", line 187, in add\n    output_tensor = layer(self.outputs[0])\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 432, in __call__\n    self.build(input_shapes[0])\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/keras/layers/convolutional.py\", line 138, in build\n    constraint=self.kernel_constraint)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 249, in add_weight\n    weight = K.variable(initializer(shape),\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/keras/initializers.py\", line 218, in __call__\n    dtype=dtype, seed=self.seed)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 4077, in random_uniform\n    dtype=dtype, seed=seed)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/tensorflow/python/ops/random_ops.py\", line 240, in random_uniform\n    shape, dtype, seed=seed1, seed2=seed2)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/tensorflow/python/ops/gen_random_ops.py\", line 449, in _random_uniform\n    name=name)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/home/ariza/.conda/envs/generative/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[4,4,128,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: conv2d_11/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=1150973, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_4/random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "smooth = 0.1\n",
    "\n",
    "\n",
    "# Adversarial ground truths\n",
    "# real = np.ones(shape=(batch_size, 1))\n",
    "# fake = np.zeros(shape=(batch_size, 1))\n",
    "real = np.ones((batch_size, 4, 4, 1))\n",
    "real = real * (1 - smooth)\n",
    "fake = np.zeros((batch_size, 4, 4, 1))\n",
    "\n",
    "fake_labels = to_categorical(np.full((batch_size, 1), num_classes), num_classes=num_classes+1)\n",
    "\n",
    "d_loss = []\n",
    "d_g_loss = []\n",
    "\n",
    "for e in range(epochs + 1):\n",
    "    for i in range(len(X_train) // batch_size):\n",
    "        \n",
    "        # Train Discriminator weights\n",
    "        discriminator.trainable = True\n",
    "        \n",
    "        # Real samples\n",
    "        img_real = X_train[i*batch_size:(i+1)*batch_size]\n",
    "        real_labels = y_train[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        d_loss_real = discriminator.train_on_batch(x=img_real, y=[real, real_labels])\n",
    "        \n",
    "        # Fake Samples\n",
    "        masked_imgs = mask_randomly(img_real)\n",
    "        gen_imgs = generator.predict(masked_imgs)\n",
    "        \n",
    "        d_loss_fake = discriminator.train_on_batch(x=gen_imgs, y=[fake, fake_labels])\n",
    "         \n",
    "        # Discriminator loss\n",
    "        d_loss_batch = 0.5 * (d_loss_real[0] + d_loss_fake[0])\n",
    "        \n",
    "        # Train Generator weights\n",
    "        discriminator.trainable = False\n",
    "\n",
    "        d_g_loss_batch = d_g.train_on_batch(x=img_real, y=real)\n",
    "   \n",
    "        print(\n",
    "            'epoch = %d/%d, batch = %d/%d, d_loss=%.3f, g_loss=%.3f' % (e + 1, epochs, i, len(X_train) // batch_size, d_loss_batch, d_g_loss_batch[0]),\n",
    "            100*' ',\n",
    "            end='\\r'\n",
    "        )\n",
    "    \n",
    "    d_loss.append(d_loss_batch)\n",
    "    d_g_loss.append(d_g_loss_batch[0])\n",
    "    print('epoch = %d/%d, d_loss=%.3f, g_loss=%.3f' % (e + 1, epochs, d_loss[-1], d_g_loss[-1]), 100*' ')\n",
    "\n",
    "    if e % 10 == 0:\n",
    "        samples = 5\n",
    "        idx = np.random.randint(0, X_train.shape[0], samples)\n",
    "        masked_imgs = mask_randomly(X_train[idx])\n",
    "        x_fake = generator.predict(masked_imgs)\n",
    "\n",
    "        for k in range(samples):\n",
    "            # plot masked\n",
    "            plt.subplot(2, 5, k+1)\n",
    "            plt.imshow(masked_imgs[k].reshape(32, 32), cmap='gray')\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            # plot recontructed\n",
    "            plt.subplot(2, 5, k+6)\n",
    "            plt.imshow(x_fake[k].reshape(32, 32), cmap='gray')\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T22:36:50.323057Z",
     "start_time": "2018-07-30T22:33:55.731Z"
    }
   },
   "outputs": [],
   "source": [
    "# plotting the metrics\n",
    "plt.plot(d_loss)\n",
    "plt.plot(d_g_loss)\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Discriminator', 'Adversarial'], loc='center right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "* [Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks](https://arxiv.org/pdf/1611.06430.pdf)\n",
    "* [How to Train a GAN? Tips and tricks to make GANs work](https://github.com/soumith/ganhacks)\n",
    "* [THE MNIST DATABASE of handwritten digits](http://yann.lecun.com/exdb/mnist/)\n",
    "* [Keras-GAN](https://github.com/eriklindernoren/Keras-GAN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
