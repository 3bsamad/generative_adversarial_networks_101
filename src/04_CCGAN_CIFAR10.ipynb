{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCGANs - Context-Conditional Generative Adversarial Networks\n",
    "\n",
    "Introduction to Context-Conditional Generative Adversarial Networks or CCGANs.\n",
    "\n",
    "This notebook is organized follows:\n",
    "\n",
    "1. **Background**\n",
    "* **Definition**\n",
    "* **Training CCGANs with CIFAR-10 dataset, Keras and TensorFlow**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background\n",
    "\n",
    "**Generative adversarial nets** consists of two models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the probability that a sample came from the training data rather than $G$.\n",
    "\n",
    "The generator distribution $p_g$ over data data $x$, the generator builds a mapping function from a prior noise distribution $p_z(z)$ to data space as $G(z;\\theta_g)$.\n",
    "\n",
    "The discriminator, $D(x;\\theta_d)$, outputs a single scalar representing the probability that $x$ came form training data rather than $p_g$.\n",
    "\n",
    "The value function $V(G,D)$:\n",
    "\n",
    "$$ \\underset{G}{min} \\: \\underset{D}{max} \\; V(D,G) = \\mathbb{E}_{x\\sim p_{data}(x)}[log D(x)] + \\mathbb{E}_{z\\sim p_{z}(z)}[log(1 - D(G(z)))]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definition\n",
    "\n",
    "Context-Conditional Generative Adversarial Networks (CC-GANs) are conditional GANs where the generator is trained to fill in a missing image patch and the generator and discriminator are conditioned on the surrounding pixels.\n",
    "\n",
    "CC-GANs address a different task: determining if a part of an image is real or fake given the surrounding context.\n",
    "\n",
    "### Generator and Discriminator\n",
    "The generator $G$ receives as input an image with a randomly masked out patch. The generator outputs an entire image.  We fill in the missing patch from the generated output and then pass the completed image into $D$.\n",
    "\n",
    "### Value function\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\underset{G}{min} \\: \\underset{D}{max} \\; V(D,G) =& \\mathbb{E}_{x\\sim \\mathcal{X}}[log D(x)] + \\mathbb{E}_{x\\sim \\mathcal{X}, m\\sim \\mathcal{M}}[log(1 - D(x_I))] \\\\\n",
    "    x_I =& (1 - m) \\bigodot x_G + m \\bigodot x \\\\\n",
    "    x_G =& G(m \\bigodot x, z)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Structure\n",
    "\n",
    "![ccgan](../img/ccgan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training CCGANs with CIFAR-10 dataset, Keras and TensorFlow\n",
    "\n",
    "CCGANs implementation using \"U-net\" model and convolutional neural network and the [Keras](https://keras.io/) library.\n",
    "\n",
    "### 1. Load data\n",
    "\n",
    "#### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T01:23:53.003672Z",
     "start_time": "2018-07-03T01:23:52.653016Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T01:23:54.731499Z",
     "start_time": "2018-07-03T01:23:53.005570Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LeakyReLU, BatchNormalization\n",
    "from keras.layers import Input, Flatten, Embedding, multiply, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras import initializers\n",
    "\n",
    "from keras_contrib.layers.normalization import InstanceNormalization\n",
    "from keras.layers import Concatenate, GaussianNoise\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras import losses\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T01:23:55.315856Z",
     "start_time": "2018-07-03T01:23:54.733836Z"
    }
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore visual data\n",
    "\n",
    "The CIFAR10 images are RGB with 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T01:23:55.757156Z",
     "start_time": "2018-07-03T01:23:55.317928Z"
    }
   },
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(y_train))\n",
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']\n",
    "\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "for i in range(num_classes):\n",
    "    ax = plt.subplot(2, 5, 1 + i, xticks=[], yticks=[])\n",
    "    idx = np.where(y_train[:]==i)[0]\n",
    "    features_idx = X_train[idx,::]\n",
    "    img_num = np.random.randint(features_idx.shape[0])\n",
    "    img = features_idx[img_num,::]\n",
    "    ax.set_title(class_names[i])\n",
    "    plt.imshow(img)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping and normalizing the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T01:23:56.191536Z",
     "start_time": "2018-07-03T01:23:55.759109Z"
    }
   },
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 3, 32, 32)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 3, 32, 32)\n",
    "    input_shape = (3, 32, 32)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], 32, 32, 3)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 32, 32, 3)\n",
    "    input_shape = (32, 32, 3)\n",
    "\n",
    "# the generator is using tanh activation, for which we need to preprocess \n",
    "# the image data into the range between -1 and 1.\n",
    "\n",
    "X_train = np.float32(X_train)\n",
    "X_train = (X_train / 255 - 0.5) * 2\n",
    "X_train = np.clip(X_train, -1, 1)\n",
    "\n",
    "X_test = np.float32(X_test)\n",
    "X_test = (X_train / 255 - 0.5) * 2\n",
    "X_test = np.clip(X_test, -1, 1)\n",
    "\n",
    "print('X_train reshape:', X_train.shape)\n",
    "print('X_test reshape:', X_test.shape)\n",
    "\n",
    "# y to categorical\n",
    "Y_train = to_categorical(y_train, num_classes=num_classes+1)\n",
    "Y_test = to_categorical(y_test, num_classes=num_classes+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define model\n",
    "\n",
    "#### Generator\n",
    "\n",
    "\"U-Net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T01:24:19.163486Z",
     "start_time": "2018-07-03T01:24:18.066195Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of filters in first layer of generator\n",
    "gf = 32\n",
    "k = 4\n",
    "s = 2\n",
    "\n",
    "# imagem shape 32x32x3\n",
    "img_shape = X_train[0].shape\n",
    "\n",
    "# Generator input\n",
    "img_g = Input(shape=(img_shape))\n",
    "\n",
    "# Downsampling\n",
    "d1 = Conv2D(gf, kernel_size=k, strides=s, padding='same')(img_g)\n",
    "d1 = LeakyReLU(alpha=0.2)(d1)\n",
    "\n",
    "d2 = Conv2D(gf*2, kernel_size=k, strides=s, padding='same')(d1)\n",
    "d2 = LeakyReLU(alpha=0.2)(d2)\n",
    "d2 = BatchNormalization(momentum=0.8)(d2)\n",
    "\n",
    "d3 = Conv2D(gf*4, kernel_size=k, strides=s, padding='same')(d2)\n",
    "d3 = LeakyReLU(alpha=0.2)(d3)\n",
    "d3 = BatchNormalization(momentum=0.8)(d3)\n",
    "\n",
    "d4 = Conv2D(gf*8, kernel_size=k, strides=s, padding='same')(d3)\n",
    "d4 = LeakyReLU(alpha=0.2)(d4)\n",
    "d4 = BatchNormalization(momentum=0.8)(d4)\n",
    "\n",
    "# Upsampling\n",
    "u1 = UpSampling2D(size=2)(d4)\n",
    "u1 = Conv2D(gf*4, kernel_size=k, strides=1, padding='same', activation='relu')(u1)\n",
    "u1 = BatchNormalization(momentum=0.8)(u1)\n",
    "\n",
    "u2 = Concatenate()([u1, d3])\n",
    "u2 = UpSampling2D(size=2)(u2)\n",
    "u2 = Conv2D(gf*2, kernel_size=k, strides=1, padding='same', activation='relu')(u2)\n",
    "u2 = BatchNormalization(momentum=0.8)(u2)\n",
    "\n",
    "u3 = Concatenate()([u2, d2])\n",
    "u3 = UpSampling2D(size=2)(u3)\n",
    "u3 = Conv2D(gf, kernel_size=k, strides=1, padding='same', activation='relu')(u3)\n",
    "u3 = BatchNormalization(momentum=0.8)(u3)\n",
    "\n",
    "u4 = Concatenate()([u3, d1])\n",
    "u4 = UpSampling2D(size=2)(u4)\n",
    "u4 = Conv2D(1, kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n",
    "\n",
    "generator = Model(img_g, u4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator model visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T01:24:19.172978Z",
     "start_time": "2018-07-03T01:24:19.165705Z"
    }
   },
   "outputs": [],
   "source": [
    "# prints a summary representation of your model\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator\n",
    "\n",
    "Our discriminator is a **convolutional neural network** that takes a 28x28 image with 1 channel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T01:24:19.325566Z",
     "start_time": "2018-07-03T01:24:19.175095Z"
    }
   },
   "outputs": [],
   "source": [
    "# Discriminator network\n",
    "k = 4\n",
    "\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Conv2D(64, kernel_size=k, strides=2, padding='same', input_shape=img_shape))\n",
    "discriminator.add(LeakyReLU(alpha=0.8))\n",
    "discriminator.add(Conv2D(128, kernel_size=k, strides=2, padding='same'))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(InstanceNormalization())\n",
    "discriminator.add(Conv2D(256, kernel_size=k, strides=2, padding='same'))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(InstanceNormalization())\n",
    "\n",
    "img_d = Input(shape=img_shape)\n",
    "features = discriminator(img_d)\n",
    "\n",
    "validity = Conv2D(1, kernel_size=k, strides=1, padding='same')(features)\n",
    "# validity = Flatten()(validity)\n",
    "# validity = Dense(1, activation='sigmoid')(validity)\n",
    "\n",
    "label = Flatten()(features)\n",
    "label = Dense(num_classes+1, activation=\"softmax\")(label)\n",
    "\n",
    "discriminator = Model(img_d, [validity, label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator model visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T01:24:19.331472Z",
     "start_time": "2018-07-03T01:24:19.327482Z"
    }
   },
   "outputs": [],
   "source": [
    "# prints a summary representation of your model\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compile model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T01:24:19.385412Z",
     "start_time": "2018-07-03T01:24:19.333720Z"
    }
   },
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "\n",
    "discriminator.compile(opt, loss=['mse', 'categorical_crossentropy'],\n",
    "                      loss_weights=[0.5, 0.5],\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T01:24:19.944323Z",
     "start_time": "2018-07-03T01:24:19.387286Z"
    }
   },
   "outputs": [],
   "source": [
    "# The generator takes noise as input and generates imgs\n",
    "masked_img = Input(shape=(img_shape))\n",
    "gen_img = generator(masked_img)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "validity, _ = discriminator(gen_img)\n",
    "\n",
    "d_g = Model(masked_img, validity)\n",
    "\n",
    "d_g.compile(opt, loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T01:24:19.951414Z",
     "start_time": "2018-07-03T01:24:19.946400Z"
    }
   },
   "outputs": [],
   "source": [
    "# prints a summary representation of your model\n",
    "d_g.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T01:24:19.958715Z",
     "start_time": "2018-07-03T01:24:19.953961Z"
    }
   },
   "outputs": [],
   "source": [
    "def mask_randomly(imgs, mask_width=10, mask_height=10):\n",
    "    y1 = np.random.randint(0, imgs.shape[1] - mask_height, imgs.shape[0])\n",
    "    y2 = y1 + mask_height\n",
    "    x1 = np.random.randint(0, imgs.shape[2] - mask_width, imgs.shape[0])\n",
    "    x2 = x1 + mask_width\n",
    "\n",
    "    masked_imgs = np.empty_like(imgs)\n",
    "    for i, img in enumerate(imgs):\n",
    "        masked_img = img.copy()\n",
    "        _y1, _y2, _x1, _x2 = y1[i], y2[i], x1[i], x2[i],\n",
    "        masked_img[_y1:_y2, _x1:_x2, :] = 0\n",
    "        masked_imgs[i] = masked_img\n",
    "\n",
    "    return masked_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T01:24:20.233329Z",
     "start_time": "2018-07-03T01:24:19.961170Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(mask_randomly(X_train[0:1])[0].reshape(32, 32))\n",
    "plt.show()\n",
    "plt.imshow(mask_randomly(X_train[0:1])[0].reshape(32, 32))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fit model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T06:03:31.509607Z",
     "start_time": "2018-07-03T01:24:20.235449Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "smooth = 0.1\n",
    "\n",
    "\n",
    "# Adversarial ground truths\n",
    "# real = np.ones(shape=(batch_size, 1))\n",
    "# fake = np.zeros(shape=(batch_size, 1))\n",
    "real = np.ones((batch_size, 4, 4, 1))\n",
    "real = real * (1 - smooth)\n",
    "fake = np.zeros((batch_size, 4, 4, 1))\n",
    "\n",
    "fake_labels = to_categorical(np.full((batch_size, 1), num_classes), num_classes=num_classes+1)\n",
    "\n",
    "d_loss = []\n",
    "d_g_loss = []\n",
    "\n",
    "for e in range(epochs + 1):\n",
    "    for i in range(len(X_train) // batch_size):\n",
    "        \n",
    "        # Train Discriminator weights\n",
    "        discriminator.trainable = True\n",
    "        \n",
    "        # Real samples\n",
    "        img_real = X_train[i*batch_size:(i+1)*batch_size]\n",
    "        real_labels = y_train[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        d_loss_real = discriminator.train_on_batch(x=img_real, y=[real, real_labels])\n",
    "        \n",
    "        # Fake Samples\n",
    "        masked_imgs = mask_randomly(img_real)\n",
    "        gen_imgs = generator.predict(masked_imgs)\n",
    "        \n",
    "        d_loss_fake = discriminator.train_on_batch(x=gen_imgs, y=[fake, fake_labels])\n",
    "         \n",
    "        # Discriminator loss\n",
    "        d_loss_batch = 0.5 * (d_loss_real[0] + d_loss_fake[0])\n",
    "        \n",
    "        # Train Generator weights\n",
    "        discriminator.trainable = False\n",
    "\n",
    "        d_g_loss_batch = d_g.train_on_batch(x=img_real, y=real)\n",
    "   \n",
    "        print(\n",
    "            'epoch = %d/%d, batch = %d/%d, d_loss=%.3f, g_loss=%.3f' % (e + 1, epochs, i, len(X_train) // batch_size, d_loss_batch, d_g_loss_batch[0]),\n",
    "            100*' ',\n",
    "            end='\\r'\n",
    "        )\n",
    "    \n",
    "    d_loss.append(d_loss_batch)\n",
    "    d_g_loss.append(d_g_loss_batch[0])\n",
    "    print('epoch = %d/%d, d_loss=%.3f, g_loss=%.3f' % (e + 1, epochs, d_loss[-1], d_g_loss[-1]), 100*' ')\n",
    "\n",
    "    if e % 10 == 0:\n",
    "        samples = 5\n",
    "        idx = np.random.randint(0, X_train.shape[0], samples)\n",
    "        masked_imgs = mask_randomly(X_train[idx])\n",
    "        x_fake = generator.predict(masked_imgs)\n",
    "\n",
    "        for k in range(samples):\n",
    "            # plot masked\n",
    "            plt.subplot(2, 5, k+1)\n",
    "            plt.imshow(masked_imgs[k].reshape(32, 32), cmap='gray')\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            # plot recontructed\n",
    "            plt.subplot(2, 5, k+6)\n",
    "            plt.imshow(x_fake[k].reshape(32, 32), cmap='gray')\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T06:03:31.684887Z",
     "start_time": "2018-07-03T06:03:31.511658Z"
    }
   },
   "outputs": [],
   "source": [
    "# plotting the metrics\n",
    "plt.plot(d_loss)\n",
    "plt.plot(d_g_loss)\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Discriminator', 'Adversarial'], loc='center right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "* [Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks](https://arxiv.org/pdf/1611.06430.pdf)\n",
    "* [How to Train a GAN? Tips and tricks to make GANs work](https://github.com/soumith/ganhacks)\n",
    "* [The CIFAR-10 dataset](https://www.cs.toronto.edu/%7Ekriz/cifar.html)\n",
    "* [Keras-GAN](https://github.com/eriklindernoren/Keras-GAN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
