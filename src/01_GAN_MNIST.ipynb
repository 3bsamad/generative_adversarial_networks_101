{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANs - GENERATIVE ADVERSARIAL NETWORKS\n",
    "\n",
    "Introduction to Generative Adversarial Networks or GANs. \n",
    "\n",
    "*\"...the most interesting idea in the last 10 years in ML\". Yann LeCun*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background\n",
    "\n",
    "Brief definition of some concepts, such as supervised and unsupervised learning, and discriminative and generative models.\n",
    "\n",
    "### Supervised learning\n",
    "\n",
    "Supervised learning algorithms learn to map a function $\\hat{y}=f(x)$, given labeled data y.\n",
    "\n",
    "* Examples: Classification algorithms (SVM), regression algorithms (Linear regression).\n",
    "\n",
    "* Applications: Object detection, semantic segmentation, image captioning, etc.\n",
    "\n",
    "### Unsupervised learning\n",
    "\n",
    "Unsupervised learning algorithms learn the underlying structure of the given data, without specifying a target value.\n",
    "\n",
    "* Examples: Clustering algorithms (k-means), generative models (GANs)\n",
    "\n",
    "* Applications: Dimensionality reduction, feature learning, density estimation, etc.\n",
    "\n",
    "### Discriminative models\n",
    "\n",
    "Most of the supervised learning algorithms are inherently discriminative. \n",
    "\n",
    "Discriminative model allows us to evaluate the **conditional probability** $P(y|x)$.\n",
    "\n",
    "Learns a function that maps the input $x$ to an output $y$.\n",
    "\n",
    "* Examples: Logistic regression, Support Vector Machines, Neural networks.\n",
    "\n",
    "### Generative models\n",
    "\n",
    "Most of the unsupervised learning algorithms are inherently generative.\n",
    "\n",
    "Generative model can allows us to evaluate the **joint probability** $P(x,y)$.\n",
    "\n",
    "Tries to learn a joint probability of the input $x$ and the output $y$ at the same time.\n",
    "\n",
    "* Examples: Latent Dirichlet allocation, Restricted Boltzmann machine, Generative adversarial networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definition\n",
    "\n",
    "[Generative Adversarial Networks or GANs](https://arxiv.org/abs/1406.2661) is a framework proposed by [Ian Goodfellow](http://www.iangoodfellow.com/), Yoshua Bengio and others in 2014.\n",
    "\n",
    "GANs are composed of two models, represented by neural networks:\n",
    "* The first model is called a **Generator** and it aims to generate new data similar to the expected one. \n",
    "* The second model is named the **Discriminator** and it aims to recognize if an input data is ‘real’ — belongs to the original dataset — or if it is ‘fake’ — generated by a forger.\n",
    "\n",
    "![GANs](https://www.kdnuggets.com/wp-content/uploads/generative-adversarial-network.png)\n",
    "\n",
    "### Generator Network\n",
    "\n",
    "The input to the generator is a series of randomly generated numbers called **latent sample**. It tries to produce data that come from some probability distribution. Without training, the generator produces garbage images only.\n",
    "\n",
    "The generator network takes random noise as input, then runs that noise through a differentiable function to transform the noise and reshape it to have recognizable structure. The output of the generator network ia a realistic image. \n",
    "\n",
    "The choice of the random input noise determines which image will come out of the generator network.\n",
    "The generator net doesn't start out producing realistic images. It has to be trained. \n",
    "GANs use an approximation where a second network, called the discriminator, learns to guide the generator.\n",
    "The generator takes random noise values Z and maps them to put values X.\n",
    "\n",
    "Now consider an adversary (G) with the mission to fool D using carefully crafted images that look almost right but not quite. This is done by picking a legitimate sample randomly from training set (latent space) and synthesizing a new image by randomly altering its features (by adding random noise). \n",
    "\n",
    "### Discriminator Network\n",
    "\n",
    "The discriminator is a **classifier** trained using the **supervised learning**. It classifies whether an image is real (1) or is fake (0).\n",
    "\n",
    "### Training GANs: Two player game \n",
    "\n",
    "The Generator (forger) needs to learn how to create data in such a way that the Discriminator isn’t able to distinguish it as fake anymore. The competition between these two teams is what improves their knowledge, until the Generator succeeds in creating realistic data.\n",
    "\n",
    "$G$ try to fool discriminator by generating real-looking images.\n",
    "\n",
    "$D$ try to distinguish between real and fake images.\n",
    "\n",
    "Train jointly in **minimax game**\n",
    "\n",
    "Minimax objetive function (Value Function of Minimax Game played by Generator and Discriminator):\n",
    "\n",
    "$$ \\underset{\\theta_{g}}{min} \\: \\underset{\\theta_{d}}{max} \\; V(D,G) = \\mathbb{E}_{x\\sim p_{data}(x)}[log D_{\\theta_{d}}(x)] + \\mathbb{E}_{z\\sim p_{z}(z)}[log(1 - D_{\\theta_{d}}(G_{\\theta_{g}}(z)))]$$\n",
    "\n",
    "* $D_{\\theta_{d}}$ wnats to maximize  objective such that $D(x)$ is close to 1 (real) and $D(G(z))$ is close to 0 (fake).\n",
    "* $G_{\\theta_{g}}$ wants to minimize objective such that $D(G(z))$ is close to 1 (discriminator is fooled into thinking generated G(z) is real).\n",
    "\n",
    "Alternate between:\n",
    "1. Gradient ascent on D\n",
    "$$\\underset{\\theta_{d}}{max} [\\mathbb{E}_{x\\sim p_{data}(x)}log D_{\\theta_{d}}(x) + \\mathbb{E}_{z\\sim p_{z}(z)}log(1 - D_{\\theta_{d}}(G_{\\theta_{g}}(z)))]$$\n",
    "\n",
    "2. Instead: Gradient ascent on generator, different objective\n",
    "$$\\underset{\\theta_{g}}{max}[\\mathbb{E}_{z\\sim p_{z}(z)}[log( D_{\\theta_{d}}(G_{\\theta_{g}}(z)))] $$\n",
    "\n",
    "Instead of minimizing likelihood of discriminator being correct, now maximize likelihood of discriminator being wrong. Same objetive of fooling discriminator, but now higher gradient signal for bad samples => works much better!\n",
    "\n",
    "As a result, \n",
    "* the Discriminator is trained to correctly classify the input data as either real or fake. \n",
    "    * This means it’s weights are updated as to maximize the probability that any real data input x is classified as belonging to the real dataset, while minimizing the probability that any fake image is classified as belonging to the real dataset. \n",
    "    * In more technical terms, the loss/error function used maximizes the function D(x), and it also minimizes D(G(z)).\n",
    "* the Generator is trained to fool the Discriminator by generating data as realistic as possible, which means that the Generator’s weight’s are optimized to maximize the probability that any fake image is classified as belonging to the real dataset. Formally this means that the loss/error function used for this network maximizes D(G(z)).\n",
    "\n",
    "### Practical tips and tricks for training\n",
    "\n",
    "[How to Train a GAN? Tips and tricks to make GANs work](https://github.com/soumith/ganhacks)\n",
    "\n",
    "It’s important to choose a good overall architecture. \n",
    "The most important design consideration for this architecture is to make sure that both the generator and the discriminator have at least one hidden layer. \n",
    "\n",
    "Two simultaneous optimizations. We define a loss for the G and D. Minimize the loss for the D, while simultaneously use another optimizer to minimize the loss for the G. \n",
    "\n",
    "Values near one for real data, and near zero for fake data.\n",
    "\n",
    "Overall this recipe of using convolutional transpose, batch normalization, Adam and Cross entropy losses with label smoothing works fairly well in practice. \n",
    "\n",
    "### Aplications \n",
    "\n",
    "GANs have already become widely known for their application versatility and their outstanding results in generating data. They have been used in real-life applications for:\n",
    "* Text generation, \n",
    "* Image generation,\n",
    "* Video generation, and \n",
    "* Text-to-image synthesis.\n",
    "\n",
    "### Most relevant GAN pros and cons\n",
    "\n",
    "Generative Adversarial Networks (GANs): Game-theoretic approach, best samples! But can be tricky and unstable to train, no inference queries.\n",
    "\n",
    "They currently generate the sharpest images\n",
    "\n",
    "They are easy to train (since no statistical inference is required), and only back-propogation is needed to obtain gradients\n",
    "\n",
    "GANs are difficult to optimize due to unstable training dynamics.\n",
    "\n",
    "No statistical inference can be done with them. (They belong to the class of direct implicit density models (they can model p(x) without explicitly defining the p.d.f))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Keras and TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
