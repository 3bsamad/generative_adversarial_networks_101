{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CGANs - Conditional Generative Adversarial Nets\n",
    "\n",
    "Brief introduction to Conditional Generative Adversarial Nets or CGANs. This notebook is organized as follows:\n",
    "\n",
    "1. **Research Paper**\n",
    "* **Background**\n",
    "* **Definition**\n",
    "* **Training CGANs with MNIST dataset, Keras and TensorFlow**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Research Paper\n",
    "\n",
    "* [Conditional Generative Adversarial Nets](https://arxiv.org/pdf/1411.1784.pdf)\n",
    "\n",
    "## 2. Background\n",
    "\n",
    "**Generative adversarial nets** consists of two models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the probability that a sample came from the training data rather than $G$.\n",
    "\n",
    "The generator distribution $p_g$ over data data $x$, the generator builds a mapping function from a prior noise distribution $p_z(z)$ to data space as $G(z;\\theta_g)$.\n",
    "\n",
    "The discriminator, $D(x;\\theta_d)$, outputs a single scalar representing the probability that $x$ came form training data rather than $p_g$.\n",
    "\n",
    "The **value function** $V(G,D)$:\n",
    "\n",
    "$$ \\underset{G}{min} \\: \\underset{D}{max} \\; V_{GAN}(D,G) = \\mathbb{E}_{x\\sim p_{data}(x)}[log D(x)] + \\mathbb{E}_{z\\sim p_{z}(z)}[log(1 - D(G(z)))]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Definition\n",
    "\n",
    "Generative adversarial nets can be extended to a **conditional model** if both the generator and discriminator are conditioned on some extra information $y$. \n",
    "\n",
    "* $y$ could be any kind of auxiliary information, such as class labels or data from other modalities. \n",
    "\n",
    "We can perform the conditioning by feeding $y$ into the both the discriminator and generator as additional input layer.\n",
    "\n",
    "* **Generator**: The prior input noise $p_z(z)$, and $y$ are combined in joint hidden representation, and the adversarial training framework allows for considerable flexibility in how this hidden representation is composed.\n",
    "\n",
    "* **Discriminator**: $x$ and $y$ are presented as inputs and to a discriminative function.\n",
    "\n",
    "### Network Design\n",
    "\n",
    "<img src=\"../../img/network_design_ccgan.png\" width=\"600\"> \n",
    "\n",
    "\n",
    "### Cost Funcion\n",
    "\n",
    "$$ \\underset{G}{min} \\: \\underset{D}{max} \\; V_{CGAN}(D,G) = \\mathbb{E}_{x\\sim p_{data}(x)}[log D(x|y)] + \\mathbb{E}_{z\\sim p_{z}(z)}[log(1 - D(G(z|y)))]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training CGANs with MNIST dataset, Keras and TensorFlow\n",
    "\n",
    "CGANs implementation using fully connected and embedding layers and the [Keras](https://keras.io/) library.\n",
    "\n",
    "* **Data**\n",
    "    * Rescale the MNIST images to be between -1 and 1.\n",
    "    \n",
    "* **Generator**\n",
    "    * **Simple fully connected neural network**, **LeakyReLU activation** and **BatchNormalization**.\n",
    "    * The input to the generator are the **normal distribution** $z$ and $ùë¶$. They are combined in joint hidden representation.\n",
    "        * Embedding($y, z$).\n",
    "    * The last activation is **tanh**.\n",
    "    \n",
    "* **Discriminator**\n",
    "    * **Simple fully connected neural network** and **LeakyReLU activation**.\n",
    "    * The input to the discriminator are $x$ and $y$. They are combined in joint hidden representation.\n",
    "        *  * Embedding($y, x$).\n",
    "    * The last activation is **sigmoid**.\n",
    "    \n",
    "* **Loss**\n",
    "    * binary_crossentropy\n",
    "\n",
    "* **Optimizer**\n",
    "    * Adam(lr=0.0002, beta_1=0.5)\n",
    "\n",
    "* batch_size = 64\n",
    "* epochs = 100\n",
    "\n",
    "### 1. Load data\n",
    "\n",
    "#### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:17:29.299849Z",
     "start_time": "2018-06-18T17:17:28.948111Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:17:30.625836Z",
     "start_time": "2018-06-18T17:17:29.301825Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LeakyReLU, BatchNormalization\n",
    "from keras.layers import Input, Flatten, Embedding, multiply, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Embedding layer background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10, 2))\n",
    "# the model will take as input an integer matrix of size (batch, input_length).\n",
    "# the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n",
    "# now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
    "\n",
    "# input_array = np.random.randint(10, size=(1, 10))\n",
    "input_array = np.arange(0, 10).reshape(1, -1)\n",
    "model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict(input_array)\n",
    "print(output_array)\n",
    "# print(output_array.shape)\n",
    "plt.scatter(output_array[0, :, 0], output_array[0, :, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:17:31.211621Z",
     "start_time": "2018-06-18T17:17:30.628121Z"
    }
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore visual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:17:31.673570Z",
     "start_time": "2018-06-18T17:17:31.213616Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    x_y = X_train[y_train == i]\n",
    "    plt.imshow(x_y[0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Class %d\" % (i))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping and normalizing the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:17:31.926590Z",
     "start_time": "2018-06-18T17:17:31.675462Z"
    }
   },
   "outputs": [],
   "source": [
    "print('X_train.shape', X_train.shape)\n",
    "print('y_train.shape', y_train.shape)\n",
    "\n",
    "# reshaping the inputs\n",
    "X_train = X_train.reshape(60000, 28*28)\n",
    "# normalizing the inputs (-1, 1)\n",
    "X_train = (X_train.astype('float32') / 255 - 0.5) * 2\n",
    "\n",
    "print('X_train reshape:', X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define model\n",
    "\n",
    "#### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:17:32.266239Z",
     "start_time": "2018-06-18T17:17:31.929037Z"
    }
   },
   "outputs": [],
   "source": [
    "# latent space dimension\n",
    "latent_dim = 100\n",
    "\n",
    "# imagem dimension 28x28\n",
    "img_dim = 784\n",
    "\n",
    "init = initializers.RandomNormal(stddev=0.02)\n",
    "\n",
    "# Generator network\n",
    "generator = Sequential()\n",
    "\n",
    "# Input layer and hidden layer 1\n",
    "generator.add(Dense(128, input_shape=(latent_dim,), kernel_initializer=init))\n",
    "generator.add(LeakyReLU(alpha=0.2))\n",
    "generator.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "# Hidden layer 2\n",
    "generator.add(Dense(256))\n",
    "generator.add(LeakyReLU(alpha=0.2))\n",
    "generator.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "# Hidden layer 3\n",
    "generator.add(Dense(512))\n",
    "generator.add(LeakyReLU(alpha=0.2))\n",
    "generator.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "# Output layer \n",
    "generator.add(Dense(img_dim, activation='tanh'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator model visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:17:32.266239Z",
     "start_time": "2018-06-18T17:17:31.929037Z"
    }
   },
   "outputs": [],
   "source": [
    "# prints a summary representation of your model\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional G model\n",
    "The prior input noise $p_z(z)$, and $y$ are combined in joint hidden representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:17:32.266239Z",
     "start_time": "2018-06-18T17:17:31.929037Z"
    }
   },
   "outputs": [],
   "source": [
    "# Embedding condition in input layer\n",
    "num_classes = 10\n",
    "\n",
    "# Create label embeddings\n",
    "label = Input(shape=(1,), dtype='int32')\n",
    "label_embedding = Embedding(num_classes, latent_dim)(label)\n",
    "label_embedding = Flatten()(label_embedding)\n",
    "\n",
    "# latent space\n",
    "z = Input(shape=(latent_dim,))\n",
    "\n",
    "# Merge inputs (z x label)\n",
    "input_generator = multiply([z, label_embedding])\n",
    "\n",
    "# Output image\n",
    "img = generator(input_generator)\n",
    "\n",
    "# Generator with condition input\n",
    "generator = Model([z, label], img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional model visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:17:32.273114Z",
     "start_time": "2018-06-18T17:17:32.268218Z"
    }
   },
   "outputs": [],
   "source": [
    "# prints a summary representation of your model\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:17:32.326856Z",
     "start_time": "2018-06-18T17:17:32.275681Z"
    }
   },
   "outputs": [],
   "source": [
    "# Discriminator network\n",
    "discriminator = Sequential()\n",
    "\n",
    "# Input layer and hidden layer 1\n",
    "discriminator.add(Dense(128, input_shape=(img_dim,), kernel_initializer=init))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "# Hidden layer 2\n",
    "discriminator.add(Dense(256))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "# Hidden layer 3\n",
    "discriminator.add(Dense(512))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "# Output layer\n",
    "discriminator.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator model visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints a summary representation of your model\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional D model\n",
    "\n",
    "$x$ and $y$ are presented as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding condition in input layer\n",
    "\n",
    "# Create label embeddings\n",
    "label_d = Input(shape=(1,), dtype='int32')\n",
    "label_embedding_d = Embedding(num_classes, img_dim)(label_d)\n",
    "label_embedding_d = Flatten()(label_embedding_d)\n",
    "\n",
    "# imagem dimension 28x28\n",
    "img_d = Input(shape=(img_dim,))\n",
    "\n",
    "# Merge inputs (img x label)\n",
    "input_discriminator = multiply([img_d, label_embedding_d])\n",
    "\n",
    "# Output image\n",
    "validity = discriminator(input_discriminator)\n",
    "\n",
    "# Discriminator with condition input\n",
    "discriminator = Model([img_d, label_d], validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional model visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints a summary representation of your model\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compile model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:17:32.385813Z",
     "start_time": "2018-06-18T17:17:32.334488Z"
    }
   },
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
    "\n",
    "discriminator.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:17:32.670427Z",
     "start_time": "2018-06-18T17:17:32.387601Z"
    }
   },
   "outputs": [],
   "source": [
    "discriminator.trainable = False\n",
    "\n",
    "validity = discriminator([generator([z, label]), label])\n",
    "\n",
    "d_g = Model([z, label], validity)\n",
    "\n",
    "d_g.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:17:32.676287Z",
     "start_time": "2018-06-18T17:17:32.672335Z"
    }
   },
   "outputs": [],
   "source": [
    "# prints a summary representation of your model\n",
    "d_g.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fit model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:50:25.368754Z",
     "start_time": "2018-06-18T17:17:32.678615Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "smooth = 0.1\n",
    "\n",
    "real = np.ones(shape=(batch_size, 1))\n",
    "fake = np.zeros(shape=(batch_size, 1))\n",
    "\n",
    "d_loss = []\n",
    "d_g_loss = []\n",
    "\n",
    "for e in range(epochs + 1):\n",
    "    for i in range(len(X_train) // batch_size):\n",
    "        \n",
    "        # Train Discriminator weights\n",
    "        discriminator.trainable = True\n",
    "        \n",
    "        # Real samples\n",
    "        X_batch = X_train[i*batch_size:(i+1)*batch_size]\n",
    "        real_labels = y_train[i*batch_size:(i+1)*batch_size].reshape(-1, 1)\n",
    "        \n",
    "        d_loss_real = discriminator.train_on_batch(x=[X_batch, real_labels], y=real * (1 - smooth))\n",
    "        \n",
    "        # Fake Samples\n",
    "        z = np.random.normal(loc=0, scale=1, size=(batch_size, latent_dim))\n",
    "        random_labels = np.random.randint(0, 10, batch_size).reshape(-1, 1)\n",
    "        X_fake = generator.predict_on_batch([z, random_labels])\n",
    "        \n",
    "        d_loss_fake = discriminator.train_on_batch(x=[X_fake, random_labels], y=fake)\n",
    "         \n",
    "        # Discriminator loss\n",
    "        d_loss_batch = 0.5 * (d_loss_real[0] + d_loss_fake[0])\n",
    "        \n",
    "        # Train Generator weights\n",
    "        discriminator.trainable = False\n",
    "        \n",
    "        z = np.random.normal(loc=0, scale=1, size=(batch_size, latent_dim))\n",
    "        random_labels = np.random.randint(0, 10, batch_size).reshape(-1, 1)\n",
    "        d_g_loss_batch = d_g.train_on_batch(x=[z, random_labels], y=real)\n",
    "   \n",
    "        print(\n",
    "            'epoch = %d/%d, batch = %d/%d, d_loss=%.3f, g_loss=%.3f' % (e + 1, epochs, i, len(X_train) // batch_size, d_loss_batch, d_g_loss_batch[0]),\n",
    "            100*' ',\n",
    "            end='\\r'\n",
    "        )\n",
    "    \n",
    "    d_loss.append(d_loss_batch)\n",
    "    d_g_loss.append(d_g_loss_batch[0])\n",
    "    print('epoch = %d/%d, d_loss=%.3f, g_loss=%.3f' % (e + 1, epochs, d_loss[-1], d_g_loss[-1]), 100*' ')\n",
    "\n",
    "    if e % 10 == 0:\n",
    "        samples = 10\n",
    "        z = np.random.normal(loc=0, scale=1, size=(samples, latent_dim))\n",
    "        labels = np.arange(0, 10).reshape(-1, 1)\n",
    "        \n",
    "        x_fake = generator.predict([z, labels])\n",
    "\n",
    "        for k in range(samples):\n",
    "            plt.subplot(2, 5, k+1)\n",
    "            plt.imshow(x_fake[k].reshape(28, 28), cmap='gray')\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:50:25.534633Z",
     "start_time": "2018-06-18T17:50:25.370944Z"
    }
   },
   "outputs": [],
   "source": [
    "# plotting the metrics\n",
    "plt.plot(d_loss)\n",
    "plt.plot(d_g_loss)\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Discriminator', 'Adversarial'], loc='center right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
